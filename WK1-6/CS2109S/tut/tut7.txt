logistic regression got sigma and two class classification model has G
decision rule g(z) positive if z mroe than 0
negative if z is negative
update weight w
for perceptron learning algorithm

sequentially process individual samples
calculate mean/gradient ad only update weight once 

use only one sample of update the weight
if there are 10 data sample
in each epoch we update the weight 10 times

logic gatesuse to model and or and not
step 1: initialize all weight to 0
w = np.zeros () 
step 2: y can only be 0 or 1
step 3 : lr = 0.1

update model for each misclassified instance
step 4: w = w + lr * ??
?? = use activation function 

use for loop to select 1 data sample to update weight
batch grad descent -> use all data samples

use another for loop to use the data more than one time iteration 10 so that every data 
sample will be used 10 times before we find correct solution

step 5:
modified sign function 
z >= 0















(a) To model the decision boundaries of the logical NOT, OR, and AND gates using perceptrons, you can set up the perceptrons as follows:

Logical NOT Gate:

Inputs: x
Weights: w = -1
Bias: b = 0
Activation function: f(z) = 1 if z >= 0 else 0
Here, the NOT gate inverts the input. If x is 0, the weighted sum (w*x + b) will be less than 0, resulting in an output of 1. If x is 1, the weighted sum will be greater than or equal to 0, resulting in an output of 0.

Logical OR Gate:

Inputs: x1, x2
Weights: w1 = 1, w2 = 1
Bias: b = -0.5
Activation function: f(z) = 1 if z >= 0 else 0
The OR gate returns 1 if at least one of the inputs is 1. The bias is set to -0.5 to account for the case where both inputs are 0. If either input is 1, the weighted sum (w1x1 + w2x2 + b) will be greater than or equal to 0, resulting in an output of 1.

Logical AND Gate:

Inputs: x1, x2
Weights: w1 = 1, w2 = 1
Bias: b = -1.5
Activation function: f(z) = 1 if z >= 0 else 0
The AND gate returns 1 only if both inputs are 1. The bias is set to -1.5 to ensure that the weighted sum (w1x1 + w2x2 + b) is greater than or equal to 0 only when both inputs are 1.


No, it is not possible to model the XOR function using a 
single perceptron. 
The XOR function's truth table is not linearly separable, 
which means there is no single straight line (decision boundary)
 that can separate the inputs into the appropriate output classes.
In a perceptron, the decision boundary is a straight line. 
The XOR function's truth table shows that when both inputs are 
0 or both inputs are 1, the output is 0, and when one input is 
0 and the other is 1, the output is 1. There is no single straight
 line that can correctly separate these two classes.

To model the XOR function, you would need a more complex neural 
network with at least one hidden layer. This can be achieved using 
multilayer perceptrons or other neural network architectures that 
can capture non-linear relationships between inputs.

(d) If you change the ordering of data samples in the Perceptron 
Update Rule, the model will still converge to the same set of 
weights for the AND operator. This is because the Perceptron 
Update Rule is a deterministic algorithm, and the order of data
amples does not affect the final weights when the data is 
linearly separable. The AND operator's weight and bias settings
will remain the same regardless of the order in which you 
present the data samples.


does order matter in batch grad descent? no
does order matter in perceptino update rule? yes bc only choose 1 data sample
order will influence number of steps and weight 
1. reorder data points can help model converge faster



Regarding Figure 1, the proposed model does not have high bias or 
high variance. The perceptron network described for modeling XOR 
is a simple model with fixed weights and biases. It has low 
variance because it does not overfit to the data, and it has 
low bias because it can accurately model the XOR function when 
given the correct inputs. High bias and high variance are typically 
associated with more complex models that may overfit or underfit 
the data, which is not the case with this perceptron network for XOR.


dont have high bias, is just right
can perfectly classify positive and negative sample
low bias = diff between predicted and real = 0 
low variance = diff between all output value and its predicted value = 0 of 0


-------------------------------------------------------------------
multi layer perceptron has lower mse loss compared to single layer perception
if multi layer perceptron still got mse, means it snot complicated enough

The single-layer perceptron can only model linear relationships 
between the input features and the output, while the multi-layer 
perceptron (a neural network with hidden layers) can capture 
non-linear patterns and more complex relationships

The single-layer perceptron may underfit the data due to its 
simplicity, while the multi-layer perceptron, with more parameters,
could be prone to overfitting if not appropriately regularized. 
Overfitting can result in poor generalization to new, unseen data.

b. 
Feature Mapping: Create new features that capture non-linear 
relationships or interactions between the existing features. 
This can help the single-layer perceptron better approximate 
the underlying function.

map input x to x to power 2
to capture complex rs 
but can lead to overfitting
if we only need x to power of 2 and we map to x to power of 10

Advantages:
Simplicity: The single-layer perceptron is straightforward and 
computationally efficient.
Interpretable: It's easier to interpret the weights and biases, 
which can provide insights into the importance of features.

Disadvantages:
Limited Expressiveness: The single-layer perceptron can't model 
complex non-linear relationships in the data.
Potential Underfitting: It may struggle with complex datasets.

(c) To improve the performance of the multi-layer perceptron, 
consider the following techniques:

Adjust Network Architecture: Add more hidden 
layers, to learn more complex patterns

increased risk of overfitting

what is the diff between adding more perceptron or adding more hidden layers
both is fine
both can add more components/parameters and learn more complex patterns
depends 
