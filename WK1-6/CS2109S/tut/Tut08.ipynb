{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Run the cell below before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(2109)\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Back Propagation with Matrix Calculus\n",
    "We provide the neural network architecture in PyTorch code below, as well as the loss function. Check your answers to parts (c) by replacing `raise NotImplementedError()` with your answer.\n",
    "\n",
    "Note that the matrix dimensions are flipped in PyTorch, where the first dimension is the number of training samples. This is just a difference in conventions, and will not affect much. Also note that we don't have the bias term in `X`, this is because the `nn.Linear` layer has it own bias included, and it will not affect your answers, since we are only asking for weight gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Define the inputs and outputs, the network, and the loss function \n",
    "'''\n",
    "n = 16  # Number of training samples\n",
    "m_0 = 8  # Number of features for each training sample\n",
    "\n",
    "X = torch.randn((n, m_0)) # random tensor of shape (n, m_0). Each row in X represents one training sample with m_0 features.\n",
    "Y = torch.randint(0, 2, (n, 1), dtype=torch.float32) # random tensor of shape (n, 1). It represents the target labels (0 or 1) for n samples.\n",
    "\n",
    "neural_net = nn.Sequential(OrderedDict([\n",
    "    ('lin1', nn.Linear(m_0, 1)), # First layer is a linear layer with m_0 input features and 1 output feature. Perform initial transformation of input features into a single value.\n",
    "    ('lin2', nn.Linear(1, 1)),  # Dummy layer so that we can illustrate part (b). It doesn't contribute anything.\n",
    "    ('sig', nn.Sigmoid()) # Applies sigmoid activation function to output a value between 0 and 1.\n",
    "]))\n",
    "\n",
    "# Set dummy layer's weight to 1 to prevent it from affecting the first layer's gradients.\n",
    "with torch.no_grad():\n",
    "    neural_net.lin2.weight[0, 0] = 1\n",
    "\n",
    "loss = nn.BCELoss() # In this question, we use binary cross entropy loss for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Obtain per-sample gradients for our dummy layer, divided by the output of `lin1`, to get the\n",
    "derivative of loss w.r.t. f.\n",
    "\n",
    "Not a common operation, but needed to illustrate part (c) of the tutorial.\n",
    "\n",
    "See https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html for more details \n",
    "about per-sample gradients and how to implement an optimized version of calculating them\n",
    "using functorch, if you're interested.\n",
    "'''\n",
    "per_sample_gradients = torch.zeros((n, 1)) # Initializes a tensor of zeros with shape (n, 1) to store per-sample gradients of the loss w.r.t dummy layer's weight\n",
    "lin1_output = torch.zeros((n, 1)) # Initialize a tensor of zeros with shape (n, 1) to store the output of the first linear layer for each training sample\n",
    "\n",
    "for i in range(n):\n",
    "    y_hat = neural_net(X[i]) # Computes forward pass through the neural network by passing the i-th sample.\n",
    "    per_sample_gradients[i] = torch.autograd.grad(loss(y_hat, Y[i]), neural_net.lin2.weight)[0] # Calculates the gradient of the loss w.r.t dummy layer's weight for the i-th sample's contribution to loss\n",
    "    lin1_output[i] = neural_net.lin1(X[i]) # Calculate the output for the first linear layer\n",
    "\n",
    "dLoss_df = 1 / n * per_sample_gradients / lin1_output # Calculate the derivative of the loss w.r.t output of the first linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Predict Y using our network, and calculate the loss of our prediction.\n",
    "\n",
    "Note that the gradients will automatically be calculated by PyTorch once you invoke the loss(...).backward() function.\n",
    "'''\n",
    "neural_net.zero_grad() # Initializes the gradients of all parameters to zero\n",
    "\n",
    "Y_hat = neural_net(X) # Make prediction Y_hat for input data X\n",
    "\n",
    "loss(Y_hat, Y).backward() # Compute the loss between predicted values Y_hat and actual target value Y, then invoke .backward() for performing back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpart_c\u001b[39m(Y, Y_hat):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# TODO: Put your answer for part (c) here\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mallclose(part_c(Y, Y_hat), dLoss_df)\n",
      "\u001b[1;32m/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpart_c\u001b[39m(Y, Y_hat):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# TODO: Put your answer for part (c) here\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eunice/Desktop/Y2S1-1/WK1-6/CS2109S/tut/Tut08.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def part_c(Y, Y_hat):\n",
    "    # TODO: Put your answer for part (c) here\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert torch.allclose(part_c(Y, Y_hat), dLoss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Potential Issues with Training Deep Neural Networks\n",
    "Run/Study the code below, and answer 3(a) and 3(b) of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create our 50-layer model.\n",
    "'''\n",
    "\n",
    "layer_dict = OrderedDict()\n",
    "for i in range(50):\n",
    "    layer_dict['lin{}'.format(i+1)] = nn.Linear(10, 10 if i < 49 else 5)\n",
    "    layer_dict['act{}'.format(i+1)] = nn.Sigmoid()\n",
    "\n",
    "deep_neural_net = nn.Sequential(layer_dict)\n",
    "\n",
    "deep_X = torch.randn(50, 10)\n",
    "deep_Y = torch.randn(50, 5)\n",
    "\n",
    "deep_Y_hat = deep_neural_net(deep_X)\n",
    "\n",
    "deep_loss = nn.L1Loss()\n",
    "deep_loss(deep_Y_hat, deep_Y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize the max gradient magnitude for the first 10 layers. Note the y-axis scale for\n",
    "each plot you generate.\n",
    "\n",
    "Feel free to play around with the method of visualization here.\n",
    "'''\n",
    "\n",
    "max_grad_magnitude_per_layer = []\n",
    "for name, layer in deep_neural_net.named_modules():\n",
    "    if isinstance(layer, nn.Linear) and int(name[3:]) < 10:\n",
    "        max_grad_magnitude_per_layer.append(torch.max(torch.abs(layer.weight.grad)))\n",
    "\n",
    "plt.plot(max_grad_magnitude_per_layer)\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Max Gradient Magnitude\")\n",
    "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1e'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0a5145e6c304e2a9afaf5b930a2955b950bd4b81fe94f7c42930f43f42762eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
